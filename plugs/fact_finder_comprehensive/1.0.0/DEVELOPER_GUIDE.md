# Fact Finder Comprehensive Plugin - Developer Guide

## Overview

The Fact Finder Comprehensive plugin is an advanced fact-checking and information verification system that validates claims, cross-references data sources, and provides comprehensive fact analysis. This plugin is designed for intelligence gathering, content validation, and automated fact verification workflows.

**Plugin Path**: `plugs/fact_finder_comprehensive/1.0.0/`
**Category**: Intelligence
**Version**: 1.0.0
**Status**: Stable
**Dependencies**: Python Standard Library only

## Core Capabilities

- **Fact Verification**: Validate claims against multiple sources
- **Source Credibility Analysis**: Assess the reliability of information sources
- **Claim Validation**: Multi-method claim verification
- **Cross-Referencing**: Compare information across multiple sources
- **Pattern Analysis**: Detect misinformation patterns
- **Temporal Consistency Checking**: Verify time-based claim consistency
- **Bulk Verification**: Process multiple claims simultaneously

## Plugin Architecture

### Verification Methods

The plugin supports multiple verification approaches:

#### 1. Cross Reference
- Compares claims against multiple authoritative sources
- Provides consensus-based verification
- Best for factual claims with clear documentation

#### 2. Pattern Analysis
- Detects known misinformation patterns
- Identifies suspicious claim structures
- Useful for detecting propaganda or fake news

#### 3. Source Credibility
- Evaluates the reliability of information sources
- Considers source reputation, bias, and accuracy history
- Essential for determining information trustworthiness

#### 4. Temporal Consistency
- Validates time-based claims for logical consistency
- Checks historical accuracy
- Useful for verifying dates, sequences, and chronological claims

#### 5. Comprehensive
- Combines all verification methods
- Provides the most thorough analysis
- Recommended for high-stakes fact-checking

### Confidence Levels

The plugin provides confidence levels to help users understand verification quality:

- **Very Low (0.0-0.25)**: Insufficient evidence or contradictory information
- **Low (0.26-0.50)**: Some supporting evidence but significant uncertainty
- **Medium (0.51-0.75)**: Reasonable evidence with minor uncertainties
- **High (0.76-1.0)**: Strong evidence with high confidence

## Configuration

### Default Configuration

```yaml
cache_enabled: true
default_confidence_threshold: 0.7
max_sources_per_claim: 10
enable_pattern_analysis: true
enable_temporal_checking: true
source_timeout_seconds: 30
```

### Configuration Options

#### Cache Settings
- `cache_enabled`: Enable result caching for improved performance
- Default: `true`

#### Verification Settings
- `default_confidence_threshold`: Minimum confidence required for positive verification
- Default: `0.7`
- Range: `0.0` to `1.0`

#### Source Management
- `max_sources_per_claim`: Maximum number of sources to analyze per claim
- Default: `10`
- Range: `1` to `50`

#### Feature Toggles
- `enable_pattern_analysis`: Enable misinformation pattern detection
- `enable_temporal_checking`: Enable temporal consistency verification
- Default: `true` for both

#### Performance Settings
- `source_timeout_seconds`: Timeout for source checking operations
- Default: `30`
- Range: `5` to `300`

## API Reference

### Core Operations

#### 1. Verify Claim
Verify a single claim against specified sources.

```python
context = {
    'action': 'verify',
    'claim': 'The Earth orbits around the Sun',
    'sources': ['nasa.gov', 'scientific-american.com'],
    'method': 'comprehensive',
    'confidence_threshold': 0.8
}

result = process(context, config)
```

**Response Structure**:
```json
{
  "success": true,
  "claim": "The Earth orbits around the Sun",
  "verified": true,
  "credibility_score": 0.95,
  "confidence_level": "high",
  "verification_method": "comprehensive",
  "evidence": [
    {
      "source": "nasa.gov",
      "supporting": true,
      "excerpt": "Earth orbits the Sun at an average distance of 93 million miles"
    }
  ],
  "sources_analyzed": 2,
  "timestamp": "2023-01-01T12:00:00Z",
  "cache_hit": false
}
```

#### 2. Cross-Check Sources
Compare information across multiple sources for consistency.

```python
context = {
    'action': 'cross_check',
    'claim': 'Water boils at 100Â°C at sea level',
    'sources': ['physics.org', 'encyclopedia.com', 'chemistry-textbook.edu']
}

result = process(context, config)
```

#### 3. Analyze Source Credibility
Evaluate the credibility and reliability of information sources.

```python
context = {
    'action': 'analyze_credibility',
    'sources': ['reliable-news.com', 'questionable-blog.net', 'academic-journal.edu']
}

result = process(context, config)
```

**Response Structure**:
```json
{
  "success": true,
  "source_analysis": [
    {
      "source": "reliable-news.com",
      "credibility_score": 0.85,
      "reliability": "high",
      "bias_assessment": "minimal",
      "factors": ["established publication", "fact-checking standards"]
    }
  ]
}
```

#### 4. Bulk Verification
Process multiple claims simultaneously for efficiency.

```python
context = {
    'action': 'bulk_verify',
    'claims': [
        'The sky is blue',
        'Water is H2O',
        'Python is a programming language'
    ],
    'sources': ['encyclopedia.com', 'scientific-sources.org']
}

result = process(context, config)
```

**Response Structure**:
```json
{
  "success": true,
  "results": [
    {
      "claim": "The sky is blue",
      "verified": true,
      "credibility_score": 0.9
    }
  ],
  "batch_summary": {
    "total_claims": 3,
    "verified_count": 3,
    "average_confidence": 0.88
  }
}
```

### Verification Methods

#### Method-Specific Usage

```python
# Cross-reference verification
context = {
    'action': 'verify',
    'claim': 'Historical fact from 1969',
    'method': 'cross_reference',
    'sources': ['history.com', 'encyclopedia.com']
}

# Pattern analysis
context = {
    'action': 'verify',
    'claim': 'Suspicious claim with potential misinformation patterns',
    'method': 'pattern_analysis',
    'sources': ['fact-checking-site.com']
}

# Temporal consistency
context = {
    'action': 'verify',
    'claim': 'Timeline-based historical claim',
    'method': 'temporal_consistency',
    'sources': ['historical-records.org']
}
```

## Integration Patterns

### Basic Fact-Checking Workflow

```python
def verify_content(claim, sources):
    """Basic fact-checking workflow"""

    # Configure for high accuracy
    config = {
        'default_confidence_threshold': 0.8,
        'enable_pattern_analysis': True,
        'max_sources_per_claim': 15
    }

    # Verify the claim
    context = {
        'action': 'verify',
        'claim': claim,
        'sources': sources,
        'method': 'comprehensive'
    }

    result = process(context, config)

    if result['success'] and result['verified']:
        return {
            'status': 'verified',
            'confidence': result['credibility_score'],
            'evidence': result['evidence']
        }
    else:
        return {
            'status': 'unverified',
            'reason': result.get('error', 'Insufficient evidence')
        }
```

### Content Moderation Integration

```python
def moderate_content(content_claims):
    """Integrate with content moderation systems"""

    # Bulk verify multiple claims
    context = {
        'action': 'bulk_verify',
        'claims': content_claims,
        'sources': [
            'fact-check.org',
            'snopes.com',
            'politifact.com'
        ],
        'confidence_threshold': 0.75
    }

    result = process(context)

    if result['success']:
        flagged_claims = [
            claim_result for claim_result in result['results']
            if not claim_result['verified']
        ]

        return {
            'moderation_status': 'flagged' if flagged_claims else 'approved',
            'flagged_claims': flagged_claims,
            'batch_summary': result['batch_summary']
        }

    return {'moderation_status': 'error', 'details': result}
```

### Research Assistant Integration

```python
def research_verification(research_claims, academic_sources):
    """Verify research claims against academic sources"""

    config = {
        'default_confidence_threshold': 0.9,  # Higher threshold for research
        'enable_temporal_checking': True,
        'max_sources_per_claim': 20
    }

    verified_research = []

    for claim in research_claims:
        context = {
            'action': 'verify',
            'claim': claim,
            'sources': academic_sources,
            'method': 'comprehensive'
        }

        result = process(context, config)

        if result['success']:
            verified_research.append({
                'claim': claim,
                'research_grade': result['confidence_level'],
                'academic_support': result['verified'],
                'evidence_quality': len(result.get('evidence', [])),
                'credibility_score': result['credibility_score']
            })

    return verified_research
```

## Error Handling

### Common Error Scenarios

#### Empty Claims
```python
context = {'action': 'verify', 'claim': ''}
result = process(context)

# Result:
{
    "success": false,
    "error": "Empty claim provided",
    "supported_actions": ["verify", "cross_check", "analyze_credibility", "bulk_verify"]
}
```

#### Invalid Actions
```python
context = {'action': 'invalid_action'}
result = process(context)

# Result:
{
    "success": false,
    "error": "Unsupported action: invalid_action",
    "supported_actions": ["verify", "cross_check", "analyze_credibility", "bulk_verify"]
}
```

#### Source Timeout
```python
# Configure shorter timeout for testing
config = {'source_timeout_seconds': 5}
context = {
    'action': 'verify',
    'claim': 'Test claim',
    'sources': ['slow-responding-source.com']
}

result = process(context, config)

# May result in:
{
    "success": true,
    "verified": false,
    "error": "Source timeout during verification",
    "sources_analyzed": 0
}
```

### Error Recovery

```python
def robust_verification(claim, sources, fallback_sources=None):
    """Robust verification with error recovery"""

    try:
        # Primary verification attempt
        result = process({
            'action': 'verify',
            'claim': claim,
            'sources': sources
        })

        if result['success']:
            return result

    except Exception as e:
        print(f"Primary verification failed: {e}")

    # Fallback to alternative sources
    if fallback_sources:
        try:
            result = process({
                'action': 'verify',
                'claim': claim,
                'sources': fallback_sources,
                'method': 'cross_reference'  # Simpler method
            })

            if result['success']:
                result['fallback_used'] = True
                return result

        except Exception as e:
            print(f"Fallback verification failed: {e}")

    # Final fallback - return unverified status
    return {
        'success': True,
        'verified': False,
        'credibility_score': 0.0,
        'confidence_level': 'very_low',
        'error': 'All verification methods failed'
    }
```

## Performance Optimization

### Caching Strategy

The plugin implements intelligent caching to improve performance:

```python
# Enable caching for repeated queries
config = {
    'cache_enabled': True,
    'default_confidence_threshold': 0.7
}

# First call - will fetch from sources
result1 = process({'action': 'verify', 'claim': 'Same claim'}, config)
print(f"Cache hit: {result1.get('cache_hit', False)}")  # False

# Second call - will use cached result
result2 = process({'action': 'verify', 'claim': 'Same claim'}, config)
print(f"Cache hit: {result2.get('cache_hit', False)}")  # True
```

### Batch Processing

For multiple claims, use bulk verification for better performance:

```python
# Efficient: Single bulk operation
context = {
    'action': 'bulk_verify',
    'claims': multiple_claims,
    'sources': common_sources
}
result = process(context)

# Less efficient: Multiple individual calls
results = []
for claim in multiple_claims:
    individual_result = process({
        'action': 'verify',
        'claim': claim,
        'sources': common_sources
    })
    results.append(individual_result)
```

### Source Management

Optimize source selection for better performance:

```python
# Prioritize fast, reliable sources
priority_sources = [
    'wikipedia.org',          # Fast, comprehensive
    'britannica.com',         # Reliable, academic
    'reuters.com'             # Current, factual
]

# Limit source count for performance
config = {
    'max_sources_per_claim': 5,  # Reduced from default 10
    'source_timeout_seconds': 15  # Shorter timeout
}
```

## Security Considerations

### Input Sanitization

The plugin automatically handles input sanitization:

```python
# Safe handling of potentially malicious input
malicious_claim = "<script>alert('xss')</script>Legitimate claim content"
context = {'action': 'verify', 'claim': malicious_claim}

result = process(context)
# Plugin safely processes the claim without executing scripts
```

### Source Validation

```python
# Plugin validates source URLs
suspicious_sources = [
    'javascript:alert("xss")',
    'file:///etc/passwd',
    'data:text/html,<script>alert(1)</script>'
]

context = {
    'action': 'verify',
    'claim': 'Test claim',
    'sources': suspicious_sources
}

result = process(context)
# Malicious sources are filtered out automatically
```

## Testing

### Comprehensive Test Suite

The plugin includes a comprehensive test suite with 20 test cases:

```bash
python /tmp/test_fact_finder_comprehensive.py
```

**Test Coverage**:
- â Plugin loading and initialization
- â Basic claim verification
- â Cross-checking multiple sources
- â Source credibility analysis
- â Bulk verification processing
- â Confidence threshold handling
- â Multiple verification methods
- â Error handling and recovery
- â Result structure validation
- â Cache functionality
- â Performance under load

### Test Results Summary
- **Total Tests**: 20
- **Success Rate**: 100%
- **Coverage**: All core functionality verified

## Development Guidelines

### Adding New Verification Methods

```python
def add_custom_method():
    """Example of extending verification methods"""

    # Custom method implementation would be added to main.py
    # Following the existing pattern for method dispatch

    context = {
        'action': 'verify',
        'claim': 'Test claim',
        'method': 'custom_method',  # Your new method
        'sources': ['source.com']
    }
```

### Extending Source Types

```python
def handle_new_source_types():
    """Example of handling new source types"""

    # Academic papers with DOI
    context = {
        'action': 'verify',
        'claim': 'Scientific claim',
        'sources': ['doi:10.1000/example-paper']
    }

    # Social media posts
    context = {
        'action': 'analyze_credibility',
        'sources': ['twitter:@username/status/123456']
    }
```

### Performance Monitoring

```python
def monitor_performance():
    """Monitor plugin performance metrics"""

    import time
    start_time = time.time()

    result = process({
        'action': 'verify',
        'claim': 'Performance test claim',
        'sources': ['test-source.com']
    })

    processing_time = time.time() - start_time

    print(f"Processing time: {processing_time:.2f}s")
    print(f"Sources analyzed: {result.get('sources_analyzed', 0)}")
    print(f"Cache hit: {result.get('cache_hit', False)}")
```

## Troubleshooting

### Common Issues

#### Low Confidence Scores
- **Cause**: Limited or contradictory source information
- **Solution**: Add more authoritative sources or lower confidence threshold

#### Slow Performance
- **Cause**: Too many sources or slow source responses
- **Solution**: Reduce `max_sources_per_claim` or `source_timeout_seconds`

#### Cache Issues
- **Cause**: Stale cached results
- **Solution**: Disable cache temporarily or implement cache invalidation

#### Source Access Errors
- **Cause**: Network issues or blocked sources
- **Solution**: Implement fallback sources and error handling

### Debug Mode

```python
def debug_verification(claim, sources):
    """Debug verification process"""

    config = {
        'cache_enabled': False,  # Disable cache for debugging
        'source_timeout_seconds': 60,  # Longer timeout
        'default_confidence_threshold': 0.5  # Lower threshold
    }

    context = {
        'action': 'verify',
        'claim': claim,
        'sources': sources,
        'method': 'comprehensive'
    }

    result = process(context, config)

    # Debug output
    print(f"Verification result: {result.get('verified')}")
    print(f"Credibility score: {result.get('credibility_score')}")
    print(f"Sources analyzed: {result.get('sources_analyzed')}")
    print(f"Evidence count: {len(result.get('evidence', []))}")

    return result
```

## Version History

### Version 1.0.0
- Initial release with comprehensive fact-checking capabilities
- Support for multiple verification methods
- Bulk processing functionality
- Caching system implementation
- Comprehensive test suite (20 tests, 100% pass rate)
- Full API documentation and developer guide

## Support and Documentation

- **Plugin Source**: `plugs/fact_finder_comprehensive/1.0.0/main.py`
- **Configuration**: `plugs/fact_finder_comprehensive/1.0.0/plug.yaml`
- **Test Suite**: `/tmp/test_fact_finder_comprehensive.py`
- **API Schema**: Defined in plug.yaml input/output schemas

For additional support, refer to the PlugPipe documentation and intelligence plugin guidelines.